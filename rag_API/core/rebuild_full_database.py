# ç®€åŒ–ç‰ˆæœ¬ - é‡æ–°åˆ›å»ºåŒ…å«å…¨éƒ¨æ•°æ®çš„å‘é‡æ•°æ®åº“
import getpass
import os
import glob
from pathlib import Path
import time
try:
    from tqdm import tqdm
except ImportError:
    def tqdm(iterable, desc="Processing", total=None):
        print(f"{desc}...")
        return iterable

from langchain_chroma import Chroma
from langchain_community.document_loaders.csv_loader import CSVLoader
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

class WeChatCSVLoader:
    """è‡ªå®šä¹‰å¾®ä¿¡èŠå¤©è®°å½•CSVåŠ è½½å™¨"""

    def __init__(self, csv_folder_path, encoding="utf-8"):
        self.csv_folder_path = Path(csv_folder_path)
        self.encoding = encoding

    def load(self):
        """åŠ è½½æ‰€æœ‰CSVæ–‡ä»¶å¹¶è¿”å›æ–‡æ¡£åˆ—è¡¨"""
        documents = []

        csv_files = list(self.csv_folder_path.glob("**/*.csv"))
        print(f"æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")

        for csv_file in tqdm(csv_files, desc="å¤„ç†CSVæ–‡ä»¶"):
            print(f"æ­£åœ¨å¤„ç†: {csv_file.name}")

            loader = CSVLoader(
                file_path=str(csv_file),
                encoding=self.encoding,
                csv_args={'delimiter': ','}
            )

            try:
                file_docs = loader.load()
                processed_count = 0
                valid_count = 0

                for doc in file_docs:
                    try:
                        content_parts = doc.page_content.split('\n')
                        chat_data = {}

                        for part in content_parts:
                            if ':' in part:
                                key, value = part.split(':', 1)
                                chat_data[key.strip()] = value.strip()

                        msg_content = chat_data.get('msg', '').strip()
                        if not msg_content:
                            continue

                        # è¿‡æ»¤æ— æ•ˆæ¶ˆæ¯
                        if (len(msg_content) <= 2 or
                            msg_content.startswith('[') or
                            msg_content.startswith('è¡¨æƒ…') or
                            'åŠ¨ç”»è¡¨æƒ…' in chat_data.get('type_name', '') or
                            msg_content == "I've accepted your friend request. Now let's chat!" or
                            '<msg>' in msg_content):
                            continue

                        formatted_content = f"""èŠå¤©è®°å½•:
æ—¶é—´: {chat_data.get('CreateTime', 'æœªçŸ¥æ—¶é—´')}
å‘é€è€…: {chat_data.get('talker', 'æœªçŸ¥ç”¨æˆ·')}
æ¶ˆæ¯ç±»å‹: {chat_data.get('type_name', 'æ–‡æœ¬')}
å†…å®¹: {msg_content}
æˆ¿é—´: {chat_data.get('room_name', 'ç§èŠ')}
æ˜¯å¦è‡ªå·±å‘é€: {'æ˜¯' if chat_data.get('is_sender') == '1' else 'å¦'}"""

                        new_doc = Document(
                            page_content=formatted_content,
                            metadata={
                                "source": str(csv_file.name),
                                "chat_time": chat_data.get('CreateTime', ''),
                                "sender": chat_data.get('talker', ''),
                                "msg_type": chat_data.get('type_name', ''),
                                "room": chat_data.get('room_name', ''),
                                "is_sender": chat_data.get('is_sender', '0'),
                                "msg_content": msg_content[:200]
                            }
                        )
                        documents.append(new_doc)
                        valid_count += 1

                    except Exception as e:
                        continue

                    processed_count += 1

                print(f"  - å¤„ç†äº† {processed_count} æ¡è®°å½•ï¼Œæœ‰æ•ˆè®°å½• {valid_count} æ¡")

            except Exception as e:
                print(f"å¤„ç†æ–‡ä»¶ {csv_file} æ—¶å‡ºé”™: {e}")
                continue

        return documents

def create_full_vectorstore(documents, embeddings, batch_size=100):
    """åˆ›å»ºåŒ…å«å…¨éƒ¨æ•°æ®çš„å‘é‡æ•°æ®åº“"""

    db_path = "./chroma_full_db"

    # åˆ é™¤æ—§æ•°æ®åº“
    if os.path.exists(db_path):
        import shutil
        print(f"åˆ é™¤æ—§æ•°æ®åº“: {db_path}")
        shutil.rmtree(db_path)

    print(f"å¼€å§‹åˆ›å»ºå®Œæ•´å‘é‡æ•°æ®åº“ï¼Œæ€»å…± {len(documents)} ä¸ªæ–‡æ¡£...")

    vectorstore = None
    failed_batches = 0
    max_retries = 3

    total_batches = (len(documents) + batch_size - 1) // batch_size

    for i in tqdm(range(0, len(documents), batch_size), desc="åˆ›å»ºå‘é‡æ•°æ®åº“", total=total_batches):
        batch = documents[i:i + batch_size]

        retry_count = 0
        while retry_count < max_retries:
            try:
                if vectorstore is None:
                    vectorstore = Chroma.from_documents(
                        documents=batch,
                        embedding=embeddings,
                        persist_directory=db_path
                    )
                    print(f"âœ… æˆåŠŸåˆ›å»ºå‘é‡æ•°æ®åº“ï¼Œç¬¬ä¸€æ‰¹ {len(batch)} ä¸ªæ–‡æ¡£")
                else:
                    vectorstore.add_documents(batch)

                vectorstore.persist()
                break

            except Exception as e:
                retry_count += 1
                print(f"æ‰¹æ¬¡ {i//batch_size + 1} å¤„ç†å¤±è´¥ (é‡è¯• {retry_count}/{max_retries}): {e}")

                if retry_count >= max_retries:
                    failed_batches += 1
                    print(f"æ‰¹æ¬¡ {i//batch_size + 1} æœ€ç»ˆå¤±è´¥ï¼Œè·³è¿‡")
                    break

                time.sleep(1)

        # æ˜¾ç¤ºè¿›åº¦
        if (i // batch_size + 1) % 10 == 0:
            completed = i + batch_size
            progress = min(completed / len(documents) * 100, 100)
            print(f"è¿›åº¦: {progress:.1f}% ({completed}/{len(documents)})")

        time.sleep(0.2)  # ç¨å¾®æ…¢ä¸€ç‚¹é¿å…è¿‡è½½

    if failed_batches > 0:
        print(f"âš ï¸ è­¦å‘Š: {failed_batches} ä¸ªæ‰¹æ¬¡å¤„ç†å¤±è´¥")

    return vectorstore

def simple_query_system(vectorstore):
    """ç®€å•çš„æŸ¥è¯¢ç³»ç»Ÿ"""
    print("\n" + "="*60)
    print("ğŸ¤– ç®€å•æŸ¥è¯¢ç³»ç»Ÿ (ä¸ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹)")
    print("âœ… ç›´æ¥è¿”å›æœ€ç›¸å…³çš„èŠå¤©è®°å½•")
    print("="*60)

    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

    while True:
        try:
            query = input("\nâ“ è¯·è¾“å…¥æŸ¥è¯¢é—®é¢˜ï¼ˆè¾“å…¥'quit'é€€å‡ºï¼‰: ").strip()

            if query.lower() in ['quit', 'exit', 'q', 'é€€å‡º']:
                print("ğŸ‘‹ å†è§ï¼")
                break

            if not query:
                print("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆé—®é¢˜")
                continue

            print(f"\nğŸ” æ­£åœ¨æŸ¥è¯¢: {query}")
            print("-" * 50)

            start_time = time.time()
            # ä½¿ç”¨similarity_search_with_scoreè·å–ç›¸ä¼¼åº¦
            results = vectorstore.similarity_search_with_score(query, k=5)
            end_time = time.time()

            print(f"ğŸ“Š æ‰¾åˆ° {len(results)} æ¡ç›¸å…³è®°å½• (è€—æ—¶: {end_time - start_time:.2f}ç§’):")

            for i, (doc, score) in enumerate(results, 1):
                print(f"\nğŸ“ è®°å½• {i} (ç›¸ä¼¼åº¦: {score:.3f}):")
                print(doc.page_content)

                # æ˜¾ç¤ºå…ƒæ•°æ®
                if 'sender' in doc.metadata:
                    print(f"ğŸ’¬ å‘é€è€…: {doc.metadata['sender']}")
                if 'chat_time' in doc.metadata:
                    print(f"â° æ—¶é—´: {doc.metadata['chat_time']}")

                print("-" * 50)

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œå†è§ï¼")
            break
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å‡ºé”™: {e}")

def main():
    """ä¸»ç¨‹åº"""
    try:
        print("ğŸš€ é‡æ–°åˆ›å»ºå®Œæ•´å‘é‡æ•°æ®åº“ç³»ç»Ÿ")
        print("ğŸ“‹ æœ¬æ¬¡å°†å¤„ç†å…¨éƒ¨èŠå¤©è®°å½•æ•°æ®")
        print("=" * 60)

        if not os.path.exists("csv"):
            print("âŒ æœªæ‰¾åˆ°csvæ–‡ä»¶å¤¹")
            return

        # æ£€æŸ¥æ˜¯å¦å·²å®‰è£…sentence-transformers
        try:
            import sentence_transformers
            print("âœ… sentence-transformers å·²å®‰è£…")
        except ImportError:
            print("âŒ ç¼ºå°‘ sentence-transformers")
            print("ğŸ’¡ è¯·è¿è¡Œ: pip install sentence-transformers")
            return

        # åŠ è½½CSVæ•°æ®
        print("\nğŸ“‚ æ­£åœ¨åŠ è½½æ‰€æœ‰CSVæ–‡ä»¶...")
        csv_loader = WeChatCSVLoader("csv")
        docs = csv_loader.load()

        if not docs:
            print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆèŠå¤©è®°å½•")
            return

        print(f"âœ… æˆåŠŸåŠ è½½ {len(docs):,} æ¡æœ‰æ•ˆèŠå¤©è®°å½•")

        # æ–‡æœ¬åˆ†å‰²
        print("\nğŸ“ æ­£åœ¨åˆ†å‰²æ–‡æ¡£...")
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=100,
            separators=["\nèŠå¤©è®°å½•:", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼Œ", " "]
        )

        splits = text_splitter.split_documents(docs)
        print(f"âœ… å·²åˆ†å‰²ä¸º {len(splits):,} ä¸ªæ–‡æœ¬ç‰‡æ®µ")

        # åˆ›å»ºæœ¬åœ°embedding
        print("\nğŸ”§ æ­£åœ¨åŠ è½½æœ¬åœ°embeddingæ¨¡å‹...")
        try:
            embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2",
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': False}
            )
            print("âœ… æœ¬åœ°embeddingæ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ embeddingæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return

        # åˆ›å»ºå®Œæ•´å‘é‡æ•°æ®åº“
        print(f"\nğŸ”§ æ­£åœ¨åˆ›å»ºåŒ…å«å…¨éƒ¨ {len(splits):,} æ¡è®°å½•çš„å‘é‡æ•°æ®åº“...")
        print("âš ï¸ è¿™å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")

        vectorstore = create_full_vectorstore(splits, embeddings, batch_size=50)

        if vectorstore is None:
            print("âŒ å‘é‡æ•°æ®åº“åˆ›å»ºå¤±è´¥")
            return

        print("âœ… å®Œæ•´å‘é‡æ•°æ®åº“åˆ›å»ºå®Œæˆï¼")

        # éªŒè¯æ•°æ®åº“
        try:
            total_count = vectorstore._collection.count()
            print(f"ğŸ“Š æ•°æ®åº“åŒ…å« {total_count:,} æ¡è®°å½•")
        except:
            print("ğŸ“Š æ•°æ®åº“åˆ›å»ºå®Œæˆï¼Œè®°å½•æ•°éªŒè¯å¤±è´¥ä½†æ•°æ®åº“å¯ç”¨")

        # å¯åŠ¨ç®€å•æŸ¥è¯¢ç³»ç»Ÿ
        simple_query_system(vectorstore)

    except Exception as e:
        print(f"âŒ ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    print("ğŸ¤– å®Œæ•´æ•°æ®å‘é‡æ•°æ®åº“åˆ›å»ºç³»ç»Ÿ")
    print("=" * 60)
    main()