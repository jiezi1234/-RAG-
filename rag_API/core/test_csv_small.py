# å°è§„æ¨¡æµ‹è¯•ç‰ˆæœ¬ - ä»…ä½¿ç”¨å‰100æ¡è®°å½•å¿«é€Ÿæµ‹è¯•
import getpass
import os
import glob
from pathlib import Path
import time
try:
    from tqdm import tqdm
except ImportError:
    def tqdm(iterable, desc="Processing", total=None):
        print(f"{desc}...")
        return iterable

# ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ï¼Œç¼ºå¤±æ—¶å†äº¤äº’å¼è¾“å…¥
os.environ["DASHSCOPE_API_KEY"] = "sk-bae62c151c524da4b4ee5f04e4e19a3f"
import dashscope
dashscope.api_key = os.environ["DASHSCOPE_API_KEY"]

from langchain_community.chat_models.tongyi import ChatTongyi

llm = ChatTongyi(model="qwen-plus")

import bs4
from langchain import hub
from langchain_chroma import Chroma
from langchain_community.document_loaders.csv_loader import CSVLoader
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_community.embeddings.dashscope import DashScopeEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

class WeChatCSVLoader:
    """è‡ªå®šä¹‰å¾®ä¿¡èŠå¤©è®°å½•CSVåŠ è½½å™¨ - å°è§„æ¨¡æµ‹è¯•ç‰ˆ"""

    def __init__(self, csv_folder_path, encoding="utf-8", max_records=100):
        self.csv_folder_path = Path(csv_folder_path)
        self.encoding = encoding
        self.max_records = max_records  # é™åˆ¶è®°å½•æ•°é‡

    def load(self):
        """åŠ è½½CSVæ–‡ä»¶å¹¶è¿”å›æœ‰é™æ•°é‡çš„æ–‡æ¡£åˆ—è¡¨"""
        documents = []
        total_processed = 0

        csv_files = list(self.csv_folder_path.glob("**/*.csv"))
        print(f"æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶ï¼Œå°†åŠ è½½å‰ {self.max_records} æ¡æœ‰æ•ˆè®°å½•")

        for csv_file in csv_files:
            if total_processed >= self.max_records:
                break

            print(f"æ­£åœ¨å¤„ç†: {csv_file.name}")

            loader = CSVLoader(
                file_path=str(csv_file),
                encoding=self.encoding,
                csv_args={'delimiter': ','}
            )

            try:
                file_docs = loader.load()
                processed_count = 0
                valid_count = 0

                for doc in file_docs:
                    if total_processed >= self.max_records:
                        break

                    try:
                        content_parts = doc.page_content.split('\n')
                        chat_data = {}

                        for part in content_parts:
                            if ':' in part:
                                key, value = part.split(':', 1)
                                chat_data[key.strip()] = value.strip()

                        msg_content = chat_data.get('msg', '').strip()
                        if not msg_content:
                            continue

                        # è¿‡æ»¤æ— æ„ä¹‰æ¶ˆæ¯
                        if (len(msg_content) <= 2 or
                            msg_content.startswith('[') or
                            msg_content.startswith('è¡¨æƒ…') or
                            'åŠ¨ç”»è¡¨æƒ…' in chat_data.get('type_name', '') or
                            msg_content == "I've accepted your friend request. Now let's chat!" or
                            '<msg>' in msg_content):
                            continue

                        # æ ¼å¼åŒ–èŠå¤©å†…å®¹
                        formatted_content = f"""èŠå¤©è®°å½•:
æ—¶é—´: {chat_data.get('CreateTime', 'æœªçŸ¥æ—¶é—´')}
å‘é€è€…: {chat_data.get('talker', 'æœªçŸ¥ç”¨æˆ·')}
æ¶ˆæ¯ç±»å‹: {chat_data.get('type_name', 'æ–‡æœ¬')}
å†…å®¹: {msg_content}
æˆ¿é—´: {chat_data.get('room_name', 'ç§èŠ')}
æ˜¯å¦è‡ªå·±å‘é€: {'æ˜¯' if chat_data.get('is_sender') == '1' else 'å¦'}"""

                        new_doc = Document(
                            page_content=formatted_content,
                            metadata={
                                "source": str(csv_file.name),
                                "chat_time": chat_data.get('CreateTime', ''),
                                "sender": chat_data.get('talker', ''),
                                "msg_type": chat_data.get('type_name', ''),
                                "room": chat_data.get('room_name', ''),
                                "is_sender": chat_data.get('is_sender', '0'),
                                "msg_content": msg_content[:200]
                            }
                        )
                        documents.append(new_doc)
                        valid_count += 1
                        total_processed += 1

                    except Exception as e:
                        continue

                    processed_count += 1

                print(f"  - å¤„ç†äº† {processed_count} æ¡è®°å½•ï¼Œæœ‰æ•ˆè®°å½• {valid_count} æ¡")

            except Exception as e:
                print(f"å¤„ç†æ–‡ä»¶ {csv_file} æ—¶å‡ºé”™: {e}")
                continue

        print(f"âœ… æ€»å…±åŠ è½½äº† {len(documents)} æ¡æœ‰æ•ˆè®°å½•ç”¨äºæµ‹è¯•")
        return documents

def create_small_vectorstore(documents, embeddings):
    """åˆ›å»ºå°è§„æ¨¡æµ‹è¯•å‘é‡æ•°æ®åº“"""

    db_path = "./chroma_wechat_db_test"

    # åˆ é™¤æ—§çš„æµ‹è¯•æ•°æ®åº“
    if os.path.exists(db_path):
        print("åˆ é™¤æ—§çš„æµ‹è¯•æ•°æ®åº“...")
        try:
            import shutil
            shutil.rmtree(db_path)
        except Exception as e:
            print(f"åˆ é™¤å¤±è´¥: {e}")

    print(f"åˆ›å»ºæµ‹è¯•å‘é‡æ•°æ®åº“ï¼Œæ–‡æ¡£æ•°é‡: {len(documents)}")

    try:
        # ä¸€æ¬¡æ€§åˆ›å»ºæ‰€æœ‰æ–‡æ¡£ï¼Œæ— éœ€åˆ†æ‰¹
        vectorstore = Chroma.from_documents(
            documents=documents,
            embedding=embeddings,
            persist_directory=db_path
        )
        print("âœ… æµ‹è¯•å‘é‡æ•°æ®åº“åˆ›å»ºæˆåŠŸ")
        return vectorstore

    except Exception as e:
        print(f"âŒ å‘é‡æ•°æ®åº“åˆ›å»ºå¤±è´¥: {e}")
        return None

def test_query(vectorstore):
    """æµ‹è¯•æŸ¥è¯¢åŠŸèƒ½"""
    print("\n" + "="*50)
    print("ğŸ” æµ‹è¯•æŸ¥è¯¢åŠŸèƒ½")
    print("="*50)

    test_queries = [
        "ä½ å¥½",
        "æ—¶é—´",
        "å­¦æ ¡",
        "ä»€ä¹ˆ",
    ]

    for query in test_queries:
        print(f"\nğŸ” æµ‹è¯•æŸ¥è¯¢: '{query}'")
        try:
            results = vectorstore.similarity_search_with_score(query, k=3)
            print(f"æ‰¾åˆ° {len(results)} æ¡ç»“æœ:")

            for i, (doc, score) in enumerate(results, 1):
                print(f"\nğŸ“ ç»“æœ {i} (ç›¸ä¼¼åº¦: {score:.3f}):")
                print(f"å‘é€è€…: {doc.metadata.get('sender', 'æœªçŸ¥')}")
                print(f"æ—¶é—´: {doc.metadata.get('chat_time', 'æœªçŸ¥')}")
                print(f"å†…å®¹: {doc.metadata.get('msg_content', '')[:50]}...")
                print("-" * 30)

        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")

    # äº¤äº’å¼æµ‹è¯•
    print("\nğŸ® äº¤äº’å¼æµ‹è¯• (è¾“å…¥ 'quit' é€€å‡º)")
    while True:
        query = input("\nâ“ è¾“å…¥æŸ¥è¯¢: ").strip()
        if query.lower() in ['quit', 'q', 'é€€å‡º']:
            break

        if not query:
            continue

        try:
            results = vectorstore.similarity_search_with_score(query, k=3)
            print(f"\næ‰¾åˆ° {len(results)} æ¡ç»“æœ:")

            for i, (doc, score) in enumerate(results, 1):
                print(f"\nğŸ“ ç»“æœ {i} (ç›¸ä¼¼åº¦: {score:.3f}):")
                print(f"å‘é€è€…: {doc.metadata.get('sender', 'æœªçŸ¥')}")
                print(f"æ—¶é—´: {doc.metadata.get('chat_time', 'æœªçŸ¥')}")
                print(f"å®Œæ•´å†…å®¹:\n{doc.page_content}")
                print("-" * 40)

        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")

def main():
    """ä¸»ç¨‹åº"""
    try:
        print("ğŸš€ å¯åŠ¨å°è§„æ¨¡æµ‹è¯•ç‰ˆå¾®ä¿¡èŠå¤©è®°å½•RAGç³»ç»Ÿ...")
        print("ğŸ“‹ æµ‹è¯•ç‰ˆç‰¹ç‚¹ï¼š")
        print("âœ… ä»…åŠ è½½å‰100æ¡æœ‰æ•ˆè®°å½•")
        print("âœ… å¿«é€Ÿåˆ›å»ºå‘é‡æ•°æ®åº“")
        print("âœ… éªŒè¯æŸ¥è¯¢ç»“æœå”¯ä¸€æ€§")
        print("="*50)

        if not os.path.exists("csv"):
            print("âŒ æœªæ‰¾åˆ°csvæ–‡ä»¶å¤¹")
            return

        # åŠ è½½å°‘é‡CSVæ•°æ®
        print("\nğŸ“‚ æ­£åœ¨åŠ è½½å°‘é‡CSVæ•°æ®...")
        csv_loader = WeChatCSVLoader("csv", max_records=100)
        docs = csv_loader.load()

        if not docs:
            print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆèŠå¤©è®°å½•")
            return

        print(f"âœ… æˆåŠŸåŠ è½½ {len(docs)} æ¡æœ‰æ•ˆèŠå¤©è®°å½•")

        # è·³è¿‡æ–‡æœ¬åˆ†å‰²ï¼Œç›´æ¥ä½¿ç”¨
        splits = docs
        print(f"âœ… ä½¿ç”¨ {len(splits)} æ¡è®°å½•åˆ›å»ºå‘é‡æ•°æ®åº“")

        # åˆ›å»ºå‘é‡æ•°æ®åº“
        print("\nğŸ”§ æ­£åœ¨åˆ›å»ºæµ‹è¯•å‘é‡æ•°æ®åº“...")
        embeddings = DashScopeEmbeddings(model="text-embedding-v3")
        vectorstore = create_small_vectorstore(splits, embeddings)

        if vectorstore is None:
            print("âŒ å‘é‡æ•°æ®åº“åˆ›å»ºå¤±è´¥")
            return

        print("âœ… æµ‹è¯•å‘é‡æ•°æ®åº“å‡†å¤‡å®Œæˆï¼")

        # æµ‹è¯•æŸ¥è¯¢
        test_query(vectorstore)

    except Exception as e:
        print(f"âŒ ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()